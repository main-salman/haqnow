# ğŸ—ï¸ HaqNow Architecture Reference

**Quick architecture reference to understand system components and data flow.**

## ğŸ—‚ï¸ **System Overview**

```
â”Œâ”€ Frontend (React) â”€â”€â”€â”€ Port 80/443 â”€â”€â”€â”€ nginx
â”œâ”€ Backend (FastAPI) â”€â”€â”€ Port 8000 â”€â”€â”€â”€â”€ Python
â”œâ”€ AI/RAG (Ollama) â”€â”€â”€â”€â”€ Port 11434 â”€â”€â”€â”€ Local LLM
â”œâ”€ MySQL Database â”€â”€â”€â”€â”€â”€ Port 21699 â”€â”€â”€â”€ Exoscale DBaaS
â”œâ”€ PostgreSQL RAG â”€â”€â”€â”€â”€ Port 21699 â”€â”€â”€â”€ Exoscale DBaaS  
â””â”€ S3 Storage â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ HTTPS â”€â”€â”€â”€â”€â”€â”€ Exoscale SOS
```

## ğŸ’¾ **Database Architecture**

### **Primary MySQL Database (Exoscale)**
- **Purpose**: Main application data
- **Tables**: documents, users, admins, translations, statistics
- **Connection**: `DATABASE_URL` in .env
- **Location**: Exoscale DBaaS (managed service)

### **RAG PostgreSQL Database (Exoscale)**  
- **Purpose**: AI/vector operations
- **Tables**: document_chunks (with embeddings), rag_queries
- **Connection**: `POSTGRES_RAG_URI` in .env
- **Extension**: pgvector for similarity search
- **Location**: Exoscale DBaaS (managed service)

## ğŸ¤– **AI/RAG Pipeline**

### **Processing Flow**
```
Document Upload â†’ Admin Approval â†’ OCR Text Extraction
     â†“
Document Chunking (500 chars) â†’ Embedding Generation (384-dim)
     â†“  
Vector Storage (PostgreSQL) â†’ Index Building (pgvector)
     â†“
Ready for AI Q&A Search
```

### **Query Flow**
```
User Question â†’ Embedding Generation â†’ Vector Similarity Search
     â†“
Context Retrieval (top 5 chunks) â†’ LLM Processing (Ollama)
     â†“
Answer + Sources + Confidence Score â†’ User Response
```

### **AI Stack Components**
- **Ollama**: Local LLM server (models: llama3, phi3:mini, gemma:2b)
- **sentence-transformers**: Embedding model (all-MiniLM-L6-v2)
- **pgvector**: PostgreSQL extension for vector operations
- **FastAPI RAG Service**: Orchestrates the entire pipeline

## ğŸŒ **Network Architecture**

### **Production Setup (Exoscale)**
```
Internet â†’ Cloudflare â†’ nginx (159.100.250.145) â†’ FastAPI (8000)
                                 â†“
                            Static Files (dist/)
                                 â†“
                            External Services:
                            â”œâ”€ MySQL DBaaS
                            â”œâ”€ PostgreSQL DBaaS  
                            â”œâ”€ S3 Object Storage
                            â””â”€ Ollama (local:11434)
```

### **Service Ports**
- **80/443**: nginx (frontend + proxy)
- **8000**: FastAPI backend (internal)
- **11434**: Ollama LLM server (internal)
- **21699**: Database connections (external)

## ğŸ“ **File System Layout**

### **Server Directory Structure**
```
/opt/foi-archive/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ .env â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Environment variables (synced from local)
â”‚   â”œâ”€â”€ main.py â”€â”€â”€â”€â”€â”€â”€â”€â”€ FastAPI application entry
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â””â”€â”€ rag_service.py â”€â”€ AI functionality
â”‚   â”‚   â”œâ”€â”€ database/
â”‚   â”‚   â”‚   â”œâ”€â”€ database.py â”€â”€â”€ MySQL connection
â”‚   â”‚   â”‚   â””â”€â”€ rag_database.py â”€ PostgreSQL connection
â”‚   â”‚   â””â”€â”€ apis/ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ API endpoints
â”‚   â””â”€â”€ requirements*.txt â”€â”€ Dependencies
â””â”€â”€ frontend/
    â””â”€â”€ dist/ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Built React application
```

### **Local Development Structure**  
```
fadih/
â”œâ”€â”€ .env â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Main environment file
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ deploy.sh â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Deployment automation
â”œâ”€â”€ documentation/
â”‚   â”œâ”€â”€ ARCHITECTURE.md â”€â”€â”€â”€ This file
â”‚   â””â”€â”€ DEBUGGING_GUIDE.md â”€â”€ Troubleshooting
â”œâ”€â”€ backend/ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ FastAPI source
â””â”€â”€ frontend/ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ React source
```

## ğŸ”„ **Data Flow**

### **Document Upload Flow**
```
1. User uploads file â†’ Frontend â†’ Backend API
2. File saved to S3 â†’ Metadata to MySQL
3. Admin approval â†’ OCR processing (Tesseract)
4. Translation (Google Translate) â†’ Text extraction
5. RAG processing â†’ Chunking + Embedding â†’ PostgreSQL
6. Available for search + AI Q&A
```

### **AI Search Flow**
```
1. User asks question â†’ Frontend â†’ Backend /api/rag/question
2. Generate query embedding â†’ sentence-transformers
3. Vector similarity search â†’ PostgreSQL pgvector
4. Retrieve top chunks â†’ Context preparation
5. LLM processing â†’ Ollama (local model)
6. Response + sources â†’ Frontend display
```

### **Traditional Search Flow**
```
1. User searches keywords â†’ Frontend â†’ Backend /api/search/search
2. Full-text search â†’ MySQL (documents table)
3. Filter by country/language â†’ Results ranking
4. Return metadata + links â†’ Frontend display
```

## ğŸš€ **Deployment Architecture**

### **Deployment Process (deploy.sh)**
```
Local Changes â†’ Git commit/push â†’ Server git pull
     â†“
Environment sync (.env) â†’ Dependency install
     â†“  
Frontend build â†’ Backend restart â†’ nginx reload
     â†“
Health checks â†’ Service verification â†’ Complete
```

### **Environment Management**
- **Local**: `.env` (main source of truth)
- **Server**: `/opt/foi-archive/backend/.env` (synced by deploy.sh)
- **Secrets**: All credentials in .env, never in code
- **Sync**: deploy.sh ensures local and server .env are identical

## ğŸ”’ **Security Architecture**

### **Authentication Flow**
- **JWT tokens**: For admin authentication
- **Password hashing**: bcrypt with salt
- **Session management**: Stateless JWT approach
- **Rate limiting**: Anonymous time-bucket system

### **Privacy Protection**
- **No IP logging**: All logs exclude IP addresses
- **Anonymous uploads**: No user identification required  
- **Encrypted storage**: S3 with encryption at rest
- **Secure transmission**: HTTPS everywhere

## ğŸ“Š **Performance Considerations**

### **Current Bottlenecks**
1. **AI processing**: 20-30 second response times
2. **Blocking operations**: AI queries block main thread
3. **Database queries**: Vector search can be slow
4. **Model loading**: Large LLM models consume memory

### **Optimization Opportunities**
1. **Async processing**: Background AI job queue
2. **Caching**: Redis for frequent queries
3. **Model optimization**: Use smaller/faster models
4. **Database tuning**: pgvector index optimization
5. **Service separation**: Dedicated AI service

### **Resource Usage**
- **RAM**: ~4GB (backend + Ollama models)
- **Storage**: ~10GB (models + documents)
- **CPU**: 2-4 cores recommended
- **Network**: External database connections