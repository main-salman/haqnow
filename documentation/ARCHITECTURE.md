# ğŸ—ï¸ HaqNow Architecture Reference

**Quick architecture reference to understand system components and data flow.**

## ğŸ—‚ï¸ **System Overview**

```
â”Œâ”€ Frontend (React) â”€â”€â”€â”€ Port 80/443 â”€â”€â”€â”€ nginx
â”œâ”€ Backend (FastAPI) â”€â”€â”€ Port 8000 â”€â”€â”€â”€â”€ Python
â”œâ”€ AI/RAG (Groq API) â”€â”€â”€ HTTPS â”€â”€â”€â”€â”€â”€â”€â”€ Cloud LLM (ultra-fast inference)
â”œâ”€ Embeddings (Local) â”€â”€ sentence-transformers â”€ Local (384-dim, multilingual)
â”œâ”€ MySQL Database â”€â”€â”€â”€â”€â”€ Port 21699 â”€â”€â”€â”€ Exoscale DBaaS
â”œâ”€ PostgreSQL RAG â”€â”€â”€â”€â”€ Port 21699 â”€â”€â”€â”€ Exoscale DBaaS  
â””â”€ S3 Storage â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ HTTPS â”€â”€â”€â”€â”€â”€â”€ Exoscale SOS
```

## ğŸ’¾ **Database Architecture**

### **Primary MySQL Database (Exoscale)**
- **Purpose**: Main application data
- **Tables**: documents, users, admins, translations, statistics
- **Connection**: `DATABASE_URL` in .env
- **Location**: Exoscale DBaaS (managed service)

### **RAG PostgreSQL Database (Exoscale)**  
- **Purpose**: AI/vector operations
- **Tables**: document_chunks (with embeddings), rag_queries
- **Connection**: `POSTGRES_RAG_URI` in .env
- **Extension**: pgvector for similarity search (384-dim vectors)
- **Location**: Exoscale DBaaS (managed service)

## ğŸ¤– **AI/RAG Pipeline**

### **Processing Flow**
```
Document Upload â†’ Admin Approval â†’ OCR Text Extraction
     â†“
Document Chunking (500 chars) â†’ Local Embedding (sentence-transformers, 384-dim)
     â†“  
Vector Storage (PostgreSQL) â†’ Index Building (pgvector)
     â†“
Ready for AI Q&A Search
```

### **Query Flow**
```
User Question â†’ Local Embedding (sentence-transformers) â†’ Vector Similarity (pgvector)
     â†“
Context Retrieval (top chunks) â†’ LLM Processing (Groq API - Mixtral, ultra-fast)
     â†“
Answer + Sources + Confidence Score â†’ User Response
```

### **AI Stack Components**
- **Groq API**: Ultra-fast cloud LLM inference (mixtral-8x7b-32768, ~2-5s responses)
- **sentence-transformers**: Local multilingual embeddings (paraphrase-multilingual-MiniLM-L12-v2, 384-dim)
- **pgvector**: PostgreSQL extension for vector similarity search
- **FastAPI RAG Service**: Orchestrates the entire pipeline

## ğŸŒ **Network Architecture**

### **Production Setup (Exoscale)**
```
Internet â†’ Deflect CDN â†’ nginx (production server) â†’ FastAPI (8000)
                                 â†“
                            Static Files (dist/)
                                 â†“
                            External Services:
                            â”œâ”€ MySQL DBaaS
                            â”œâ”€ PostgreSQL DBaaS  
                            â”œâ”€ S3 Object Storage
                            â””â”€ Groq API (LLM only)
```

### **Service Ports**
- **80/443**: nginx (frontend + proxy)
- **8000**: FastAPI backend (internal)
- **21699**: Database connections (external)
- **External APIs**: Groq (LLM only) via HTTPS

## ğŸ“ **File System Layout**

### **Server Directory Structure**
```
/opt/foi-archive/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ .env â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Environment variables (synced from local)
â”‚   â”œâ”€â”€ main.py â”€â”€â”€â”€â”€â”€â”€â”€â”€ FastAPI application entry
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â””â”€â”€ rag_service.py â”€â”€ AI functionality
â”‚   â”‚   â”œâ”€â”€ database/
â”‚   â”‚   â”‚   â”œâ”€â”€ database.py â”€â”€â”€ MySQL connection
â”‚   â”‚   â”‚   â””â”€â”€ rag_database.py â”€ PostgreSQL connection
â”‚   â”‚   â””â”€â”€ apis/ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ API endpoints
â”‚   â””â”€â”€ requirements*.txt â”€â”€ Dependencies
â””â”€â”€ frontend/
    â””â”€â”€ dist/ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Built React application
```

### **Local Development Structure**  
```
fadih/
â”œâ”€â”€ .env â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Main environment file
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ deploy.sh â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Deployment automation
â”œâ”€â”€ documentation/
â”‚   â”œâ”€â”€ ARCHITECTURE.md â”€â”€â”€â”€ This file
â”‚   â””â”€â”€ DEBUGGING_GUIDE.md â”€â”€ Troubleshooting
â”œâ”€â”€ backend/ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ FastAPI source
â””â”€â”€ frontend/ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ React source
```

## ğŸ”„ **Data Flow**

### **Document Upload Flow**
```
1. User uploads file â†’ Frontend â†’ Backend API
2. File saved to S3 â†’ Metadata to MySQL
3. Admin approval â†’ OCR processing (Tesseract)
4. Translation (Google Translate) â†’ Text extraction
5. RAG processing â†’ Chunking + Embedding â†’ PostgreSQL
6. Available for search + AI Q&A
```

### **AI Search Flow**
```
1. User asks question â†’ Frontend â†’ Backend /api/rag/question
2. Generate query embedding â†’ sentence-transformers (local, 384-dim)
3. Vector similarity search â†’ PostgreSQL pgvector (cosine similarity)
4. Retrieve top chunks â†’ Context preparation
5. LLM processing â†’ Groq API (mixtral-8x7b-32768 - ultra-fast)
6. Response + sources â†’ Frontend display
```

### **Traditional Search Flow**
```
1. User searches keywords â†’ Frontend â†’ Backend /api/search/search
2. Full-text search â†’ MySQL (documents table)
3. Filter by country/language â†’ Results ranking
4. Return metadata + links â†’ Frontend display
```

## ğŸš€ **Deployment Architecture**

### **Deployment Process (deploy.sh)**
```
Local Changes â†’ Git commit/push â†’ Server git pull
     â†“
Environment sync (.env) â†’ Dependency install
     â†“  
Frontend build â†’ Backend restart â†’ nginx reload
     â†“
Health checks â†’ Service verification â†’ Complete
```

### **Environment Management**
- **Local**: `.env` (main source of truth)
- **Server**: `/opt/foi-archive/backend/.env` (synced by deploy.sh)
- **Secrets**: All credentials in .env, never in code
- **Sync**: deploy.sh ensures local and server .env are identical

## ğŸ”’ **Security Architecture**

### **Authentication Flow**
- **JWT tokens**: For admin authentication
- **Password hashing**: bcrypt with salt
- **Session management**: Stateless JWT approach
- **Rate limiting**: Anonymous time-bucket system

### **Privacy Protection**
- **No IP logging**: All logs exclude IP addresses
- **Anonymous uploads**: No user identification required  
- **Encrypted storage**: S3 with encryption at rest
- **Secure transmission**: HTTPS everywhere

## ğŸ“Š **Performance Considerations**

### **Performance Improvements (Hybrid Architecture)**
1. **AI processing**: ~2-5 second response times (was 20-30s with Ollama)
2. **Non-blocking**: All operations are async
3. **Groq speed**: Up to 625 tokens/second (50-100x faster than Ollama)
4. **Low memory**: sentence-transformers uses ~500MB RAM (vs 4GB+ for Ollama)
5. **Cost**: $0 with Groq free tier (no OpenAI costs)

### **Optimization Opportunities**
1. **Caching**: Redis for frequent queries and embeddings
2. **Database tuning**: pgvector index optimization
3. **Batch embeddings**: sentence-transformers supports efficient batching
4. **Model caching**: sentence-transformers models cached after first load

### **Resource Usage**
- **RAM**: ~1.5GB (backend + sentence-transformers model)
- **Storage**: ~2GB (documents + 500MB embedding model)
- **CPU**: 2 cores sufficient (embeddings are CPU-bound but fast)
- **Network**: Groq API + database connections only